{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data from here https://www.kaggle.com/c/boston-housing/overview\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/shestakoff/hse_se_ml/master/2020/s11-boosting/data/boston_train.csv').drop(columns = 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prediction of house price in Boston.\n",
    " \n",
    " Why it's important? Because it can help people determine a fair price at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'medv').values\n",
    "y = df['medv'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definition of huber_loss: https://en.wikipedia.org/wiki/Huber_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that calculates $z_i$ from gradient boosting algorithm (minus gradient of loss function). Implement for all possible loss functions (mse, hl, logloss)\n",
    "\n",
    "In hl use $\\delta = 1$, assume that $MSE = \\frac{1}{2}(r - y)^2$ and log_loss(sigmoid(r)) use $0$ and $1$ like class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_objective(r, target, loss = 'mse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        r : np.array\n",
    "            value of f(x)\n",
    "        target : np.array\n",
    "            target\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl(https://en.wikipedia.org/wiki/Huber_loss, logloss(sigmoid(r))\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    delta = 1\n",
    "    \n",
    "    if(loss=='logloss'):\n",
    "        import math\n",
    "        \n",
    "        for i in range(r.shape[0]):\n",
    "            sig_r = 1 / (1 + math.exp(-r[i] * target[i]))\n",
    "            if(target[i] == 0):\n",
    "                ret.append(- 1 / (1 + math.exp(-r[i])))\n",
    "            else:\n",
    "                ans = (math.exp(-r[i] * target[i]) * target[i])\n",
    "                ret.append(ans * sig_r)\n",
    "        return ret\n",
    "    \n",
    "    for i in range(r.shape[0]):\n",
    "        if(abs(r[i] - target[i]) > delta and loss=='hl'):\n",
    "                l =  -(r[i] - target[i])/abs(r[i] - target[i])\n",
    "        else:\n",
    "            l = -(r[i] - target[i])\n",
    "        ret.append(l)\n",
    "    return ret\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your code\n",
    "\n",
    "r = np.array([1, 2, 3, 4, 5])\n",
    "target = np.array([10, 9, 8, 7, 6])\n",
    "assert(np.allclose(calc_objective(r, target, 'mse'), np.array([9, 7, 5, 3, 1]), 0.00001))\n",
    "\n",
    "r = np.array([2, 4, 7, 9, 13])\n",
    "target = np.array([2.5, 6, 10, 6, 12.75])\n",
    "assert(np.allclose(calc_objective(r, target, 'hl'), np.array([0.5, 1, 1, -1, -0.25]), 0.00001))\n",
    "\n",
    "r = np.array([0, np.log(2), -np.log(9), np.log(4), np.log(19)])\n",
    "target = np.array([0, 1, 0, 1, 1])\n",
    "assert(np.allclose(calc_objective(r, target, 'logloss'), np.array([-0.5, 1. / 3, -0.1, 0.2, 0.05]), 0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function which make one step of gradient boossting (fit new estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, y, r, base_estimator, loss = 'mse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the target variables\n",
    "        r : np.array, shape = (n_ojects)\n",
    "            f_{m-1}(X) (X matrix of features) - prediction of previous m-1 base_estimators\n",
    "        base_estimator : estimator which you must fit\n",
    "            has got method fit\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl, logloss\n",
    "            \n",
    "        Return fitted base_estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    z = - calc_objective(r, y, loss)\n",
    "    \n",
    "    base_estimator.fit(X, z)\n",
    "    \n",
    "    return base_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use your knowledge and implement gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (6 points)\n",
    "\n",
    "Implement GradientBoostingRegressor as it was described in your lectures:\n",
    "\n",
    "**Input**: training dataset $(x_{i},y_{i}),\\,i=1,2,...N$; loss function $\\mathcal{L}(f,y)$; learning rate $\\nu$ and the number $M$ of successive additive approximations.\n",
    "\n",
    "1. Fit initial approximation $f_{0}(x)$ (might be taken $f_{0}(x)\\equiv0$)\n",
    "2. For each step $m=1,2,...M$:\n",
    "\n",
    "    1. calculate derivatives $z_{i}=-\\frac{\\partial\\mathcal{L}(r,y_{i})}{\\partial r}|_{r=f^{m-1}(x_{i})}$\n",
    "    2. fit $h_{m}$ to $\\{(x_{i},z_{i})\\}_{i=1}^{N}$, for example by solving\n",
    "$$\n",
    "\\sum_{n=1}^{N}(h_{m}(x_{n})-z_{n})^{2}\\to\\min_{h_{m}}\n",
    "$$\n",
    "    4. set $f_{m}(x)=f_{m-1}(x)+\\nu h_{m}(x)$\n",
    "\n",
    "\n",
    "**Output**: approximation function $f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M}\\nu h_{m}(x)$\n",
    "\n",
    "Implement three loss functions:\n",
    "\n",
    "    1 MSE\n",
    "    2 Huber loss(https://en.wikipedia.org/wiki/Huber_loss)\n",
    "    3 log_loss (in this case we solve classification task\n",
    "In our case $h_m$ is DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to modify this class #\n",
    "\n",
    "class GradientBoostingEstimator(object):\n",
    "\n",
    "    def __init__(self, n_estimators, max_depth = 3, max_leaf_nodes = 8, n_subset_features = 5, random_state = 42,\n",
    "                 loss = 'mse', learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        This is your random forest classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int\n",
    "            Number of estimators to train.\n",
    "        max_depth : int\n",
    "            max_depth of DecisionTreeRegressor\n",
    "        max_leaf_nodes:\n",
    "            max_leaf_nodes of DecisionTreeRegressor\n",
    "        n_subset_features : int\n",
    "            Number of random features to used to train a decision tree\n",
    "        random_state : int\n",
    "            random_state for decision tree\n",
    "        loss : str\n",
    "            Loss. Possible values : mse, hl, logloss\n",
    "        learning_rate : float\n",
    "            learning_rate (coef for next estimator on each step)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_estimators = n_estimators\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.n_subset_features = n_subset_features\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.nu = learning_rate\n",
    "        \n",
    "        self.regressors = []\n",
    "    \n",
    "    \n",
    "    def calc_objective(self, r, target, loss = 'mse'):\n",
    "        \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        r : np.array\n",
    "            value of f(x)\n",
    "        target : np.array\n",
    "            target\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl(https://en.wikipedia.org/wiki/Huber_loss, logloss(sigmoid(r))\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        delta = 1\n",
    "    \n",
    "        if(loss=='logloss'):\n",
    "            import math\n",
    "        \n",
    "            for i in range(r.shape[0]):\n",
    "                sig_r = 1 / (1 + math.exp(-r[i] * target[i]))\n",
    "                if(target[i] == 0):\n",
    "                    ret.append(- 1 / (1 + math.exp(-r[i])))\n",
    "                else:\n",
    "                    ans = (math.exp(-r[i] * target[i]) * target[i])\n",
    "                    ret.append(ans * sig_r)\n",
    "            return ret\n",
    "    \n",
    "        for i in range(r.shape[0]):\n",
    "            if(abs(r[i] - target[i]) > delta and loss=='hl'):\n",
    "                l =  (r[i] - target[i])/abs(r[i] - target[i])\n",
    "            else:\n",
    "                l = -(r[i] - target[i])\n",
    "            ret.append(l)\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the object labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        #Initial fit\n",
    "        \n",
    "        self.r = np.zeros(X.shape[0], dtype=float) #f_{0}(X)\n",
    "        \n",
    "        #1-M fittings\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "               \n",
    "            rgr = DecisionTreeRegressor(max_depth=self.max_depth, max_leaf_nodes=self.max_leaf_nodes, \n",
    "                                                 random_state = self.random_state)\n",
    "            \n",
    "            self.z = self.calc_objective(self.r, y, self.loss)\n",
    "            \n",
    "            rgr.fit(X, self.z)\n",
    "            \n",
    "            self.regressors.append(rgr)\n",
    "            \n",
    "            self.r = self.r + self.nu * rgr.predict(X)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # TODO\n",
    "            #tree_value = rgr.tree_.value\n",
    "                        \n",
    "            #leaf_ids = rgr.apply(X)\n",
    "\n",
    "            #tree_values_gammas = np.zeros(leaf_ids.shape[0])\n",
    "            \n",
    "            \n",
    "            #for l in leaf_ids:\n",
    "                \n",
    "                #for k in tree_value[l]:\n",
    "                    #tree_values_gammas[l] = tree_values_gammas[l] + k\n",
    "                \n",
    "                #tree_values_gammas[l] = tree_values_gammas[l] / tree_value[l].shape[0]\n",
    "            \n",
    "            \n",
    "            #for j in range(r.shape[0]):\n",
    "                #self.r[j] = self.r[j] + self.nu * tree_values_gammas[leaf_ids[j]]\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs labels prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        labels : numpy.array, shape = (n_objects)\n",
    "            1D array with predicted labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        res = np.zeros(X.shape[0])\n",
    "        \n",
    "        for regressor in self.regressors:\n",
    "            res = res + self.nu * regressor.predict(X)\n",
    "            \n",
    "        return res\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs probabilities prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : numpy.array, shape = (n_objects, n_classes)\n",
    "            Array with predicted probabilities. \n",
    "        \"\"\"\n",
    "        \n",
    "        res = np.zeros(X.shape[0])\n",
    "        \n",
    "        for regressor in self.regressors:\n",
    "            res = res + self.nu * regressor.predict(X)\n",
    "            \n",
    "        res = 1/(1+np.exp(-res))\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.0718562874251497\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingEstimator(n_estimators=100, loss = 'mse')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=123) \n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y1 = clf.predict(X_test).astype(int)\n",
    "y2 = y_test.astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_test = accuracy_score(y1,y2)\n",
    "print(\"Test accuracy: \", accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "When you select `learning_rate` and `n_estimators`, follow the default values of the tree parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (2 points)\n",
    "\n",
    "Split your data on train, valid sample (fix random_seed). Choose the best `learning_rate` and `n_estimators` for every loss (for logloss use data from previous HW). For regression task (mse_loss, huber_loss) use MSE on valid sample as quality measure to choose optimal params, for classification task (logloss) use ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.1 30.1 14.3 17.7 22.8 37.9 20.1 22.8 20.6 18.2 20.4 19.3 13.8 21.7\n",
      " 23.1 13.3 23.4 23.7 21.5 43.1 16.4 36.1 13.5 32.4 27.5 14.1 20.3 19.7\n",
      " 24.4 19.1 20.4 36.2 25.2 22.6 23.9 28.  36.5 19.8 18.9  8.7 23.8 13.8\n",
      " 19.1 25.  20.6 46.  37.6 19.6 35.4 13.3  8.4 21.4 22.8 33.  20.1 22.6\n",
      " 24.5 23.3 19.4 26.4 21.2 20.2 24.4 23.7 23.9 19.4 11.3 23.9 16.1 15.6\n",
      " 15.4 27.1 16.2 15.  28.4 50.  14.5 16.6 22.2 12.  19.5 33.1 20.8 19.4\n",
      " 20.8 30.8 27.9 50.  20.  17.8 23.3 17.1 35.4 41.7 20.3 28.5 20.1 23.7\n",
      " 21.7 22.9 18.4 25.  13.5 19.8 50.  25.3 19.3 16.3 23.2 24.4 19.2 12.5\n",
      " 37.2 23.6 22.2 17.2 22.  17.5 22.7 42.8 23.  24.7 16.1 19.6 24.7 13.9\n",
      "  9.7 10.5 17.8 16.1 21.7 32.7 22.3  8.1 10.8 17.6 31.1 28.1 26.2 24.\n",
      " 48.3 23.4 31.6 18.9 34.9 17.8 13.6 19.9 21.4 20.  22.  29.9 12.3 29.1\n",
      " 31.2 29.  25.  18.6 34.9 17.8  5.  14.1 18.8 12.8 32.2 50.  23.2]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "roc_auc_score() missing 1 required positional argument: 'y_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-91e77b8c7a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mrfc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: roc_auc_score() missing 1 required positional argument: 'y_score'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.5,\n",
    "random_state=123)\n",
    "\n",
    "results = []\n",
    "\n",
    "n_estimators = list(range(1, 100))\n",
    "\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "max_score = 0\n",
    "best_learning_rate = 0\n",
    "best_n_estimators = 0\n",
    "\n",
    "#MSE\n",
    "#for lr in learning_rate:\n",
    "    #for n in n_estimators:\n",
    "        #clf = GradientBoostingEstimator(learning_rate=lr, n_estimators=n)\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #proba = clf.predict_proba(X_test)\n",
    "        #rfc_auc = roc_auc_score(proba, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = GradientBoostingEstimator(learning_rate=0.1, n_estimators=100, loss='logloss')\n",
    "clf.fit(X_train, y_train)\n",
    "proba = clf.predict_proba(X_test)\n",
    "print(y_test)\n",
    "rfc_auc = roc_auc_score(proba, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (1 point)\n",
    "Plot dependecy of loss value (in classification task plot roc-auc score) from `n_estimators` of your boosting. Use `learning_rate=0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (1 point)\n",
    "Plot dependecy of loss value (in classification task plot roc-auc score) from `learning_rate` of your boosting. Use `n_estimators=100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.5,\n",
    "random_state=123)\n",
    "\n",
    "clf = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
